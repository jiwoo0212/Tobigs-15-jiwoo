{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning 에서는 Q값을 Q테이블 형태로 구했다면 DQN에서는 Q값을 뉴럴넷을 이용해서 구합니다. \n",
    "\n",
    "상태(state)가 너무 많을때에는 각 상태에 대한 Q값을 테이블 형태로 나타내는 것이 매우 힘들기 때문에, \n",
    "\n",
    "상태(s)가 input, 가능한 행동(a)들에 대한 Q값을 output으로 가지는 뉴럴넷을 학습하는 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://greentec.github.io/images/rl2_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴럴넷을 이용한 RL에는 문제점이 존재합니다.\n",
    "\n",
    "1. 뉴럴넷 학습에는 label이 필요한데, RL은 보상을 통해서 학습이 이뤄지고, 보상은 sparse/noisy/delayed 되는 경우가 많다.\n",
    "\n",
    "2. `corelations between samples (샘플간의 깊은 상관성)`\n",
    "input으로 들어오는 상태들은 상관성이 큽니다.\n",
    "\n",
    "이에 대한 해결책은 다음과 같았습니다. \n",
    "\n",
    "`experience replay`\n",
    "들어오는 상태를 버퍼에 저장해놓은 후에 여기에서 랜덤하게 미니배치를 샘플링하여 학습을 하는 것입니다. \n",
    "\n",
    "이 방법을 사용하면 한 부분에 집중된 sample이 아니라 전체적으로 랜덤하게 샘플링되어서 상관성이 큰 input 문제를 해결할 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://sanghyukchun.github.io/images/post/90-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구체적인 알고리즘 원리는 위와 같습니다. \n",
    "\n",
    "인풋을 전처리하고 -> E-greedy 방법을 이용해서 action을 선택하고  \n",
    "\n",
    "-> 이에 따른 reward와 다음 state를 받아서 전처리 후 D(버퍼)에 저장 (experience replay)\n",
    "\n",
    "-> 버퍼에서 미니배치를 뽑아서 gradient descent를 이용해 가중치를 업데이트 합니다. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
