{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tobig's 15기 2주차 Optimization 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent 구현하기\n",
    "\n",
    "### 1)\"...\"표시되어 있는 빈 칸을 채워주세요\n",
    "### 2)강의내용과 코드에 대해 공부한 내용을 마크마운 또는 주석으로 설명해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  \n",
    "똑같이 X_test에다 fit하면 안돼요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62384804, 0.44459787, 0.46164952])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "random_parameters = parameters.copy()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * LaTeX   \n",
    "\n",
    "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
    "LaTeX문법으로 아래의 수식을 완성해주세요  \n",
    "http://triki.net/apps/3466  \n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n",
    "## $z = X_i \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)):\n",
    "        z += np.dot(X[i], parameters[i])\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test!\n",
    "xx = np.array([[1,1,1]])\n",
    "p = np.array([1])\n",
    "dot_product(xx,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1712441312275954"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bias          1.000000\n",
       "experience    1.185555\n",
       "salary        0.043974\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p = \"{1 \\over1+e^{-X_i\\theta}}\"$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    z = dot_product(X,parameters)\n",
    "    p =  1+np.exp(1)**-z\n",
    "    return 1/p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.763369824666022"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "<br>\n",
    "선형 회귀의 목적함수\n",
    "## $l(\\theta) = \\frac{1}{2}\\Sigma(y_i - \\theta^{T}X_i)^2$  \n",
    "참고) $\\hat{y_i} = \\theta^{T}X_i$\n",
    "  \n",
    "로지스틱 회귀의 목적함수를 작성해주세요  \n",
    "(선형 회귀의 목적함수처럼 강의에 나온대로 작성해주세요. 평균을 고려하는 것은 뒤에 코드에서 수행합니다)\n",
    "## $l(p) = - \\Sigma\\{y_ilogp(X_i)+(1-y_i)log(1-p(X_i))\\}$\n",
    "## $     = - \\Sigma\\{y_iX_i\\theta - log(1+e^{X_i\\theta})\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_log_cross_entropy_i(X, y, parameters):\n",
    "    p = logistic(X, parameters)\n",
    "    loss = -(y*(np.log(p)) - (1-y)*(np.log(1-p)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression cost function\n",
    "def mse_i(X, y, parameters):\n",
    "    y_hat = dot_product(X, parameters)\n",
    "    loss = (y - y_hat)**2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(X_set, y_set, parameters, loss_function, n): #n:현재 배치의 데이터 수\n",
    "    loss = 0\n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        loss += loss_function(X, y, parameters)\n",
    "    loss = loss/X_set.shape[0] #loss 평균값으로 계산\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9795781061994577"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_loss(X_test, y_test, parameters, minus_log_cross_entropy_i, len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "위의 선형회귀의 목적함수 $l(\\theta)$와 로지스틱회귀의 목적함수 $l(p)$의 gradient를 작성해주세요  \n",
    "(위의 목적함수를 참고해서 작성해주세요 = 평균을 고려하는 것은 뒤에 코드에서 수행합니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ${\\partial\\over{\\partial \\theta_j}}l(\\theta)=\\Sigma(y_i - \\theta^TX_i)X_{ij}$ \n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)= -\\Sigma(y_i - p_i)X_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_ij(X, y, parameters, j, model): # 데이터 하나(i)의 한 계수(j)에 대한 편미분 값\n",
    "    if model == 'linear':\n",
    "        y_hat = dot_product(X, parameters)\n",
    "        gradient = (y - y_hat)*X[j]\n",
    "    else:\n",
    "        p = logistic(X, parameters)\n",
    "        gradient = (y-p)*X[j]\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08570734090500352"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15485143603936488"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij(X_train.iloc[0,:], y_train.iloc[0], parameters, 1, 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient\n",
    "하나의 배치 (X_set, y_set)에 대해 기울기를 구하는 코드를 작성해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient(X_set, y_set, parameters, model):\n",
    "    gradients = [0 for _ in range(len(parameters))]\n",
    "    \n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i,:]\n",
    "        y = y_set.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij(X, y, parameters, j, model) \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-52.26044205715597, -6.067355516819472, -37.91925140231375]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients1 = batch_gradient(X_train, y_train, parameters, 'logistic')\n",
    "gradients1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-50.57536850524622, -99.22373535945218, -132.01012017102656]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients2 = batch_gradient(X_train, y_train, parameters, 'linear')\n",
    "gradients2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini-batch\n",
    "인덱스로 미니 배치 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_idx(X_train, batch_size):\n",
    "    N = len(X_train)\n",
    "    nb = (N // batch_size)+1 #number of batch\n",
    "    idx = np.array([i for i in range(N)])\n",
    "    idx_list = [idx[i*batch_size:(i+1)*batch_size] for i in range(nb) if len(idx[i*batch_size:(i+1)*batch_size]) != 0]\n",
    "    return idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_idx 함수에 대한 설명을 batch_size와 함께 간략하게 작성해주세요  \n",
    "##### 설명: data를 batch_size 만큼 인덱스를 떼내서 반환해주는 함수입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters\n",
    "기울기를 갱신하는 코드를 작성해주세요  \n",
    "(loss와 마찬가지로 기울기를 갱신할 때 배치 사이즈를 고려해 평균으로 갱신해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(parameters, gradients, learning_rate, n): #n:현재 배치의 데이터 수\n",
    "    for i in range(len(parameters)): \n",
    "        parameters[i] += learning_rate*gradients[i]/n\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61368509, 0.44122395, 0.45603825])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(parameters, gradients1, 0.01, len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "위에서 작성한 함수들을 조합해서 경사하강법 함수를 완성해주세요\n",
    "\n",
    "- learning_rate: 학습률  \n",
    "- tolerance: Step이 너무 작아서 더 이상의 학습이 무의미할 때 학습을 멈추는 조건  \n",
    "- batch: 기울기를 1번 갱신할 때 사용하는 데이터셋  \n",
    "- epoch: 모든 train data를 학습하는 것\n",
    "- num_epoch: 에폭 수\n",
    "<br>\n",
    "\n",
    "BGD: batch gradient descent 학습 한 번에 모든 데이터셋에 대해 기울기를 구한다.\n",
    "\n",
    "SGD: stochastic gradient descent 학습 한 번에 랜덤으로 선택한 하나의 데이터에 대한 기울기를 구한다.\n",
    "\n",
    "MGD: mini batch gradient descent 전체 데이터의 일부(배치)에 대해 기울기를 구한다.\n",
    "\n",
    "<br>\n",
    "batch_size에 따른 경사하강법의 종류를 적어주세요 \n",
    "\n",
    "batch_size=1 -> SGD \n",
    "\n",
    "batch_size=k -> MGD  \n",
    "\n",
    "batch_size=whole -> BGD  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'logistic', batch_size = 16):\n",
    "    stopper = False\n",
    "    \n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.random.rand(N)\n",
    "    loss_function = minus_log_cross_entropy_i if model == 'logistic' else mse_i\n",
    "    loss = 999\n",
    "    batch_idx_list = batch_idx(X_train, batch_size)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        if stopper:\n",
    "            break\n",
    "        for idx in batch_idx_list:\n",
    "            X_batch = X_train.iloc[idx,]\n",
    "            y_batch = y_train.iloc[idx]\n",
    "            gradients = batch_gradient(X_batch, y_batch, parameters, model)\n",
    "            parameters = step(parameters, gradients, learning_rate, len(X_batch))\n",
    "            new_loss = batch_loss(X_batch, y_batch, parameters, loss_function, len(X_batch))\n",
    "            \n",
    "            #중단 조건\n",
    "            if abs(new_loss - loss) < tolerance:\n",
    "                stopper = True\n",
    "                print('break')\n",
    "                break\n",
    "            loss = new_loss\n",
    "        \n",
    "        #100epoch마다 학습 상태 출력\n",
    "        if epoch%100 == 0: #출력이 길게 나오면 check point를 수정해도 됩니다.\n",
    "            print(f\"epoch: {epoch}  loss: {-new_loss}  params: {parameters}  gradients: {gradients}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n",
    "경사하강법 함수를 이용해 최적의 모수 찾아보세요. 학습을 진행할 때, Hyper Parameter를 바꿔가면서 학습시켜보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 1.0923811829912407  params: [0.50373879 0.39083668 0.25178703]  gradients: [-3.5584285242147824, -3.211632674400558, -4.098629582650231]\n",
      "epoch: 100  loss: 0.07596272750064754  params: [-1.56005057  3.06645742 -3.01420332]  gradients: [-0.4447517530475609, -0.6550196680031499, -0.8719036848810479]\n",
      "epoch: 200  loss: 0.07400304271970876  params: [-1.77324997  3.74516464 -3.65495873]  gradients: [-0.4183425078710976, -0.6337595596546741, -0.7842769670390735]\n",
      "epoch: 300  loss: 0.07352257435391395  params: [-1.86812649  4.04197175 -3.93273214]  gradients: [-0.4113250893566607, -0.6266882467282522, -0.7532786211210689]\n",
      "epoch: 400  loss: 0.07334224994194881  params: [-1.91609393  4.19118734 -4.07184383]  gradients: [-0.4085670994399378, -0.6235467137483407, -0.7391174895008442]\n",
      "break\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.9243604 ,  4.22971122, -4.08459784])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning_rate = 0.1\n",
    "new_param_bgd1 = gradient_descent(X_train, y_train, learning_rate = 0.1, num_epoch = 1000, tolerance = 0.000001, model = 'logistic', batch_size = 16)\n",
    "new_param_bgd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.7612554549256594  params: [0.87032538 0.09477383 0.29438351]  gradients: [-11.500423394434893, -2.5320549448379084, -10.458957885849903]\n",
      "epoch: 100  loss: -0.04623642479861075  params: [-1.07999086  1.56330304 -1.47764531]  gradients: [0.523609214596977, 0.14761631219599397, -3.008011470251158]\n",
      "epoch: 200  loss: -0.00732114498987867  params: [-1.27798391  2.30020854 -2.19021865]  gradients: [0.1285643611276563, -0.20694264856625744, -2.060563150528451]\n",
      "epoch: 300  loss: 0.015227539517493265  params: [-1.40671286  2.74934061 -2.61896976]  gradients: [-0.1478353794622209, -0.3707214322761336, -1.6329775421376194]\n",
      "epoch: 400  loss: 0.03007713749676509  params: [-1.49864938  3.05956082 -2.91259214]  gradients: [-0.33767112076766304, -0.4642626997141883, -1.3868005350743144]\n",
      "epoch: 500  loss: 0.040708508793087766  params: [-1.56742762  3.2881713  -3.12767397]  gradients: [-0.47539898291157934, -0.5248312162583518, -1.2263100802283564]\n",
      "epoch: 600  loss: 0.0486925039953719  params: [-1.62056297  3.46334623 -3.29175358]  gradients: [-0.5791433704398223, -0.5670802115142494, -1.1137286715557526]\n",
      "epoch: 700  loss: 0.05487624228111928  params: [-1.66255716  3.6010886  -3.4203371 ]  gradients: [-0.659407050794607, -0.5980131215162273, -1.0308966717487085]\n",
      "epoch: 800  loss: 0.059769737787528525  params: [-1.69629983  3.71138468 -3.52302907]  gradients: [-0.7227496762319571, -0.6214372922250669, -0.9678983371136474]\n",
      "epoch: 900  loss: 0.06370359430324038  params: [-1.72375023  3.80089084 -3.6061909 ]  gradients: [-0.7735027999704547, -0.6396170179855845, -0.9188149548468092]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.74609006,  3.87359876, -3.67363248])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning_rate = 0.05\n",
    "new_param_bgd2 = gradient_descent(X_train, y_train, learning_rate = 0.05, num_epoch = 1000, tolerance = 0.000001, model = 'logistic', batch_size = 30)\n",
    "new_param_bgd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 0.8586078869765809  params: [0.74259206 0.50280173 0.9284522 ]  gradients: [-10.608733428370135, -5.851366692483692, -14.780034982357149]\n",
      "epoch: 100  loss: -0.0653869835520784  params: [-0.92847093  1.04528697 -0.96593361]  gradients: [0.5198582992945531, 0.412834776867035, -3.9463835671403067]\n",
      "epoch: 200  loss: -0.03888384605590678  params: [-1.12239549  1.71659089 -1.62589911]  gradients: [0.4659048048908535, 0.07159844866372747, -2.7759741496442256]\n",
      "epoch: 300  loss: -0.014724965477965412  params: [-1.2399314   2.16129373 -2.05604361]  gradients: [0.21342592090454698, -0.14471552685128114, -2.2089325329775606]\n",
      "epoch: 400  loss: 0.0020543948944144945  params: [-1.33048742  2.48543425 -2.36691745]  gradients: [0.015380285365443419, -0.2758787068959043, -1.8692353033648712]\n",
      "epoch: 500  loss: 0.014491438142453478  params: [-1.40288729  2.73582005 -2.6055062 ]  gradients: [-0.1386656068069541, -0.3634241952580114, -1.6415625853534266]\n",
      "epoch: 600  loss: 0.024183831704964565  params: [-1.4620606   2.93652865 -2.79578659]  gradients: [-0.2616178442346648, -0.4261346684493156, -1.4776584357408218]\n",
      "epoch: 700  loss: 0.031991307349490766  params: [-1.5112889   3.10154929 -2.95159147]  gradients: [-0.361887522733926, -0.4733137442348096, -1.3538029324971839]\n",
      "epoch: 800  loss: 0.038426535500612306  params: [-1.55283364  3.23974078 -3.08162531]  gradients: [-0.4450427481575751, -0.5100792402137424, -1.2569121056310035]\n",
      "epoch: 900  loss: 0.04382085946609059  params: [-1.58829996  3.35707693 -3.19172415]  gradients: [-0.5149315768747152, -0.5394909418170252, -1.179132739257977]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.61857635,  3.45684673, -3.28511863])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning rate = 0.03\n",
    "new_param_bgd3 = gradient_descent(X_train, y_train, learning_rate = 0.03, num_epoch = 1000, tolerance = 0.000000001, model = 'logistic', batch_size = 30)\n",
    "new_param_bgd3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate = 0.03\n",
    "y_predict = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_bgd3)\n",
    "    if p> 0.5 :\n",
    "        y_predict.append(1)\n",
    "    else :\n",
    "        y_predict.append(0)\n",
    "y_predict_random = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], random_parameters)\n",
    "    if p> 0.5 :\n",
    "        y_predict_random.append(1)\n",
    "    else :\n",
    "        y_predict_random.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38,  2],\n",
       "       [ 2,  8]], dtype=int64)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn) / (tp+fn+fp+tn)\n",
    "print(\"accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "### $y = 0.5 + 2.7x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_X = np.random.rand(150)\n",
    "y = 2.7*raw_X + 0.5 + np.random.randn(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([1 for _ in range(150)])\n",
    "X = np.vstack((tmp, raw_X)).T\n",
    "X = pd.DataFrame(X)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87084815, 2.21548779])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규방정식\n",
    "theta = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: -1.2944219024745114  params: [1.24726297 0.92358104]  gradients: [2.1866792771989236, 2.9993373124442204]\n",
      "epoch: 100  loss: -0.9043725724734173  params: [0.83464073 2.18713998]  gradients: [0.0532527665305107, 0.9108744791014467]\n",
      "epoch: 200  loss: -0.9043579650836577  params: [0.83459725 2.18722318]  gradients: [0.053298120004275695, 0.9108251817621618]\n",
      "epoch: 300  loss: -0.9043579640388438  params: [0.83459725 2.18722319]  gradients: [0.05329812324837668, 0.9108251782359544]\n",
      "epoch: 400  loss: -0.90435796403877  params: [0.83459725 2.18722319]  gradients: [0.05329812324861116, 0.9108251782357079]\n",
      "epoch: 500  loss: -0.90435796403877  params: [0.83459725 2.18722319]  gradients: [0.05329812324861116, 0.9108251782357079]\n",
      "epoch: 600  loss: -0.90435796403877  params: [0.83459725 2.18722319]  gradients: [0.05329812324861116, 0.9108251782357079]\n",
      "epoch: 700  loss: -0.90435796403877  params: [0.83459725 2.18722319]  gradients: [0.05329812324861116, 0.9108251782357079]\n",
      "epoch: 800  loss: -0.90435796403877  params: [0.83459725 2.18722319]  gradients: [0.05329812324861116, 0.9108251782357079]\n",
      "epoch: 900  loss: -0.90435796403877  params: [0.83459725 2.18722319]  gradients: [0.05329812324861116, 0.9108251782357079]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.83459725, 2.18722319])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#경사하강법\n",
    "new_param = gradient_descent(X, y,learning_rate = 0.1, num_epoch = 1000, tolerance = 0.00001, model = 'linear', batch_size = 10)\n",
    "new_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_NE = theta.dot(X.T)\n",
    "y_hat_GD = new_param.dot(X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "시각화를 통해 정규방정식과 경사하강법을 통한 선형회귀를 비교해보세요  \n",
    "(밑의 코드를 실행만 시키면 됩니다. 추가 코드 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X.iloc[:,1], y, '.k') #산점도\n",
    "plt.plot(X.iloc[:,1], y_hat_NE, '-b', label = 'NE') #정규방정식\n",
    "plt.plot(X.iloc[:,1], y_hat_GD, '-r', label = 'GD') #경사하강법\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
